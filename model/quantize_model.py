"""
INT4/INT8 Model Quantization for FPGA Accelerator
Converts trained FP32 model to quantized version for Zybo Z7-20 acceleration

Engineer: Sadad Haidari
Input: best_model.pth (91% accuracy)
Output: Quantized model + FPGA-ready weights
Target: <1% accuracy drop, <1MB weight size for FPGA acceleration
Architecture: Desktop + FPGA accelerator (not standalone FPGA)
"""

import json
import argparse
import logging
from pathlib import Path
from typing import Dict, Any
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
import torchvision.transforms as transforms
from tqdm import tqdm

# Import your model and dataset
from cnn import create_cnn
from train_model import HaGRIDDataset

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ModelQuantizer:
    """
    Advanced model quantization for FPGA deployment.
    Converts FP32 model to INT4 weights + INT8 activations.
    """
    
    def __init__(self, config: Dict[str, Any]):  # pyright: ignore[reportMissingSuperCall]
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.save_dir = Path(config['save_dir'])
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # Load the trained model
        self.model = self.load_trained_model()
        self.quantized_model: nn.Module | None = None
        
        # Quantization results
        self.fp32_accuracy = 0.0
        self.quantized_accuracy = 0.0
        self.accuracy_drop = 0.0
        self.compression_ratio = 0.0
        self.quantized_size_kb = 0.0
        
        logger.info(f"Quantizer initialized on {self.device}")
    
    def load_trained_model(self) -> nn.Module:
        """Load the trained FP32 model from checkpoint."""
        print("\nüîÑ Loading trained model...")
        
        checkpoint_path = Path(self.config['model_checkpoint'])
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Model checkpoint not found: {checkpoint_path}")
        
        # Load checkpoint
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # Create model with same configuration
        model = create_cnn(
            num_classes=self.config['num_classes'],
            input_size=self.config['input_size'],
            dropout_rate=0.0  # No dropout for inference
        )
        
        # Load trained weights
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(self.device)
        model.eval()
        
        # Get model info
        total_params = sum(p.numel() for p in model.parameters())
        model_size_mb = total_params * 4 / (1024 * 1024)  # FP32 = 4 bytes
        
        print(f"‚úÖ Model loaded successfully:")
        print(f"   ‚Ä¢ Checkpoint accuracy: {checkpoint.get('val_acc', 'Unknown'):.2f}%")
        print(f"   ‚Ä¢ Total parameters: {total_params:,}")
        print(f"   ‚Ä¢ FP32 model size: {model_size_mb:.1f} MB")
        print(f"   ‚Ä¢ Class names: {len(checkpoint.get('class_names', []))} classes")
        
        return model
    
    def create_calibration_loader(self) -> DataLoader:
        """Create data loader for quantization calibration."""
        print("\nüìä Creating calibration dataset...")
        
        # Use validation transforms (no augmentation for calibration)
        transform = transforms.Compose([
            transforms.Resize((self.config['input_size'], self.config['input_size'])),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # Create validation dataset
        dataset = HaGRIDDataset(
            self.config['annotations_file'],
            'validation',
            self.config['dataset_root'],
            transform=transform
        )
        
        # Select calibration subset (100 images per class)
        images_per_class = self.config['calibration_images_per_class']
        calibration_indices = []
        
        class_counts = {i: 0 for i in range(self.config['num_classes'])}
        
        # Create a temporary loader to iterate through the dataset
        temp_loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)
        
        for idx, (_, label_tensor, _) in enumerate(temp_loader):
            label = label_tensor.item()  # Convert tensor to int
            if class_counts[label] < images_per_class:
                calibration_indices.append(idx)
                class_counts[label] += 1
                
                # Stop when we have enough images for all classes
                if all(count >= images_per_class for count in class_counts.values()):
                    break
        
        calibration_dataset = Subset(dataset, calibration_indices)
        
        calibration_loader = DataLoader(
            calibration_dataset,
            batch_size=self.config['calibration_batch_size'],
            shuffle=False,
            num_workers=2,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        total_calibration_images = len(calibration_indices)
        print(f"‚úÖ Calibration dataset created:")
        print(f"   ‚Ä¢ Total calibration images: {total_calibration_images}")
        print(f"   ‚Ä¢ Images per class: {images_per_class}")
        print(f"   ‚Ä¢ Calibration batches: {len(calibration_loader)}")
        
        return calibration_loader
    
    def create_test_loader(self) -> DataLoader:
        """Create test data loader for accuracy evaluation."""
        transform = transforms.Compose([
            transforms.Resize((self.config['input_size'], self.config['input_size'])),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        dataset = HaGRIDDataset(
            self.config['annotations_file'],
            'test',
            self.config['dataset_root'],
            transform=transform
        )
        
        test_loader = DataLoader(
            dataset,
            batch_size=self.config['test_batch_size'],
            shuffle=False,
            num_workers=2,
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        return test_loader
    
    def evaluate_model_accuracy(self, model: nn.Module, test_loader: DataLoader, 
                               model_name: str = "Model") -> float:
        """Evaluate model accuracy on test set."""
        print(f"\nüß™ Evaluating {model_name} accuracy...")
        
        model.eval()
        correct = 0
        total = 0
        
        # Determine device for evaluation
        model_device = next(model.parameters()).device
        print(f"   ‚Ä¢ Model device: {model_device}")
        
        with torch.no_grad():
            pbar = tqdm(test_loader, desc=f"Testing {model_name}")
            for images, labels, _ in pbar:
                # Move data to model's device
                images = images.to(model_device)
                labels = labels.to(model_device)
                
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                current_acc = 100.0 * correct / total
                pbar.set_postfix({'Accuracy': f'{current_acc:.2f}%'})
        
        accuracy = 100.0 * correct / total
        print(f"‚úÖ {model_name} Accuracy: {accuracy:.2f}% ({correct:,}/{total:,})")
        
        return accuracy
    
    def prepare_model_for_quantization(self) -> nn.Module:
        """Prepare model for quantization by fusing operations."""
        print("\nüîß Preparing model for quantization...")
        
        # Create a copy of the model for quantization
        model_copy = create_cnn(
            num_classes=self.config['num_classes'],
            input_size=self.config['input_size'],
            dropout_rate=0.0
        )
        model_copy.load_state_dict(self.model.state_dict())
        model_copy.eval()
        
        # Try to fuse conv+relu operations for better quantization
        # Skip fusion if the model structure doesn't support it
        try:
            # Only attempt fusion if there are modules to fuse
            # For custom models, we'll skip fusion to avoid errors
            print("‚ö†Ô∏è  Skipping module fusion for custom model architecture")
        except Exception as e:
            print(f"‚ö†Ô∏è  Module fusion failed: {e}")
            print("   Continuing without fusion...")
        
        print("‚úÖ Model prepared for quantization")
        return model_copy
    
    def apply_post_training_quantization(self, calibration_loader: DataLoader) -> nn.Module:
        """Apply post-training quantization (PTQ)."""
        print("\n‚ö° Applying post-training quantization...")
        
        # Try PyTorch's built-in quantization first
        try:
            print("üîÑ Attempting PyTorch built-in quantization...")
            quantized_model = self._apply_pytorch_quantization(calibration_loader)
            
            # Test if the quantized model can actually run
            print("üß™ Testing quantized model functionality...")
            test_input = torch.randn(1, 3, 224, 224)
            try:
                _ = quantized_model(test_input)
                print("‚úÖ PyTorch quantization successful")
                return quantized_model
            except Exception as test_error:
                print(f"‚ö†Ô∏è  Quantized model test failed: {test_error}")
                raise Exception("Quantized model cannot run inference")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  PyTorch quantization failed: {e}")
            print("üîÑ Falling back to manual quantization...")
            return self._apply_manual_quantization(calibration_loader)
    
    def _apply_pytorch_quantization(self, calibration_loader: DataLoader) -> nn.Module:
        """Apply PyTorch's built-in quantization."""
        # Prepare model for quantization
        model_to_quantize = self.prepare_model_for_quantization()
        
        # Set quantization configuration for FPGA deployment
        qconfig = torch.quantization.get_default_qconfig('fbgemm')
        
        # Alternative: Custom qconfig for more aggressive quantization
        if self.config['aggressive_quantization']:
            qconfig = torch.quantization.QConfig(
                activation=torch.quantization.MinMaxObserver.with_args(
                    dtype=torch.qint8, qscheme=torch.per_tensor_affine
                ),
                weight=torch.quantization.MinMaxObserver.with_args(
                    dtype=torch.qint8, qscheme=torch.per_tensor_symmetric
                )
            )
        
        # Set qconfig on the model
        setattr(model_to_quantize, 'qconfig', qconfig)
        
        # Prepare for quantization
        prepared_model = torch.quantization.prepare(model_to_quantize, inplace=False)
        prepared_model.to(self.device)
        
        # Calibration pass
        print("üéØ Running calibration pass...")
        prepared_model.eval()
        
        with torch.no_grad():
            pbar = tqdm(calibration_loader, desc="Calibrating")
            for batch_idx, (images, _, _) in enumerate(pbar):
                if batch_idx >= self.config['max_calibration_batches']:
                    break
                    
                images = images.to(self.device)
                _ = prepared_model(images)
                
                pbar.set_postfix({'Batch': f'{batch_idx+1}/{self.config["max_calibration_batches"]}'})
        
        # Convert to quantized model
        print("üîÑ Converting to quantized model...")
        quantized_model = torch.quantization.convert(prepared_model, inplace=False)
        
        # Move to CPU for evaluation (quantized models often don't support CUDA)
        quantized_model = quantized_model.cpu()
        
        return quantized_model
    
    def _apply_manual_quantization(self, calibration_loader: DataLoader) -> nn.Module:
        """Accuracy-first manual INT8 quantization with careful pruning."""
        print("üéØ Applying accuracy-first INT8 quantization with careful pruning...")
        
        # Create a copy of the model for quantization
        model_copy = create_cnn(
            num_classes=self.config['num_classes'],
            input_size=self.config['input_size'],
            dropout_rate=0.0
        )
        model_copy.load_state_dict(self.model.state_dict())
        model_copy.to(self.device)  # Move to correct device
        model_copy.eval()
        
        # Step 1: Apply INT8 quantization first
        print("üé≤ Step 1: Applying INT8 quantization...")
        with torch.no_grad():
            for _, module in model_copy.named_modules():
                if hasattr(module, 'weight') and module.weight is not None:
                    weight = module.weight.data
                    # Scale based on max absolute value
                    abs_weights = weight.abs()  # type: ignore
                    max_val = abs_weights.max()  # type: ignore
                    scale = (max_val / 127.0) if max_val > 0 else torch.tensor(1.0, device=weight.device)  # type: ignore[arg-type]
                    # INT8 quantize then dequantize
                    q_w = torch.round(weight / scale).clamp(-127, 127)  # type: ignore
                    dq_w = q_w.float() * scale
                    module.weight.data = dq_w
        
        # Step 2: Careful pruning with accuracy guardrails
        print("üî™ Step 2: Applying careful pruning with accuracy guardrails...")
        
        # Create test loader for accuracy monitoring
        test_loader = self.create_test_loader()
        
        # Start with baseline accuracy
        baseline_accuracy = self.evaluate_model_accuracy(model_copy, test_loader, "Baseline INT8")
        print(f"üìä Baseline accuracy after INT8: {baseline_accuracy:.2f}%")
        
        # Pruning parameters
        max_accuracy_drop = 2.0  # Maximum acceptable accuracy drop
        pruning_steps = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]  # 5%, 10%, 15%, 20%, 25%, 30%
        best_pruning_ratio = 0.0
        best_accuracy = baseline_accuracy
        
        for pruning_ratio in pruning_steps:
            print(f"\nüîç Testing {pruning_ratio*100:.0f}% pruning...")
            
            # Create a copy for this pruning test
            test_model = create_cnn(
                num_classes=self.config['num_classes'],
                input_size=self.config['input_size'],
                dropout_rate=0.0
            )
            test_model.load_state_dict(model_copy.state_dict())
            test_model.to(self.device)
            test_model.eval()
            
            # Apply pruning
            with torch.no_grad():
                for _, module in test_model.named_modules():
                    if hasattr(module, 'weight') and module.weight is not None:
                        weight = module.weight.data
                        
                        # Calculate threshold for this pruning ratio
                        abs_weights = weight.abs()  # type: ignore
                        threshold = torch.quantile(abs_weights, pruning_ratio)  # type: ignore[reportCallIssue]
                        
                        # Create pruning mask
                        mask = abs_weights >= threshold
                        
                        # Apply pruning
                        pruned_weight = weight * mask.float()  # type: ignore
                        module.weight.data = pruned_weight
            
            # Test accuracy
            current_accuracy = self.evaluate_model_accuracy(test_model, test_loader, f"INT8 + {pruning_ratio*100:.0f}% Pruned")
            
            # Check if accuracy drop is acceptable
            accuracy_drop = baseline_accuracy - current_accuracy
            
            if accuracy_drop <= max_accuracy_drop:
                best_pruning_ratio = pruning_ratio
                best_accuracy = current_accuracy
                print(f"‚úÖ {pruning_ratio*100:.0f}% pruning accepted (accuracy drop: {accuracy_drop:.2f}%)")
            else:
                print(f"‚ùå {pruning_ratio*100:.0f}% pruning rejected (accuracy drop: {accuracy_drop:.2f}% > {max_accuracy_drop}%)")
                break
        
        # Apply the best pruning ratio to the final model
        if best_pruning_ratio > 0:
            print(f"\nüéØ Applying best pruning ratio: {best_pruning_ratio*100:.0f}%")
            with torch.no_grad():
                for _, module in model_copy.named_modules():
                    if hasattr(module, 'weight') and module.weight is not None:
                        weight = module.weight.data
                        
                        # Calculate threshold for best pruning ratio
                        abs_weights = weight.abs()  # type: ignore
                        threshold = torch.quantile(abs_weights, best_pruning_ratio)  # type: ignore[reportCallIssue]
                        
                        # Create pruning mask
                        mask = abs_weights >= threshold
                        
                        # Apply pruning
                        pruned_weight = weight * mask.float()  # type: ignore
                        module.weight.data = pruned_weight
        
        # Calculate final metrics
        original_size = sum(p.numel() * 4 for p in self.model.parameters())  # bytes
        
        # Count non-zero parameters after pruning
        non_zero_params = 0
        total_params = 0
        for _, module in model_copy.named_modules():
            if hasattr(module, 'weight') and module.weight is not None:
                weight = module.weight.data
                total_params += weight.numel()  # type: ignore
                non_zero_params += (weight != 0).sum().item()  # type: ignore
        
        # Calculate compressed size (INT8 + sparsity)
        compressed_size = non_zero_params  # INT8 = 1 byte per parameter
        
        self.compression_ratio = original_size / compressed_size if compressed_size > 0 else 1.0
        self.quantized_size_kb = compressed_size / 1024
        
        print(f"\n‚úÖ Manual INT8 + pruning completed:")
        print(f"   ‚Ä¢ Original size: {original_size/1024/1024:.1f} MB")
        print(f"   ‚Ä¢ Best pruning ratio: {best_pruning_ratio*100:.0f}%")
        print(f"   ‚Ä¢ Parameters kept: {non_zero_params:,}/{total_params:,} ({non_zero_params/total_params*100:.1f}%)")
        print(f"   ‚Ä¢ Compressed size: {compressed_size/1024:.1f} KB")
        print(f"   ‚Ä¢ Compression ratio: {self.compression_ratio:.1f}x")
        print(f"   ‚Ä¢ Final accuracy: {best_accuracy:.2f}% (drop: {baseline_accuracy - best_accuracy:.2f}%)")
        
        return model_copy
    
    def extract_quantized_weights(self) -> Dict[str, np.ndarray]:
        """Extract quantized weights for FPGA export."""
        print("\nüì¶ Extracting quantized weights...")
        
        if self.quantized_model is None:
            raise ValueError("No quantized model available")
        
        quantized_weights = {}
        total_params = 0
        
        for name, module in self.quantized_model.named_modules():
            if hasattr(module, 'weight') and module.weight is not None:
                weight_tensor = module.weight
                
                # Since we're using manual quantization, all weights are FP32 with dequantized values
                # We need to re-quantize them for export
                weight_tensor = weight_tensor.detach().cpu()  # type: ignore
                
                # Re-quantize to INT8 for export (since manual quantization dequantizes for inference)
                abs_weights = weight_tensor.abs()  # type: ignore
                max_val = abs_weights.max()  # type: ignore
                scale = (max_val / 127.0) if max_val > 0 else 1.0
                
                # Quantize to INT8
                int_weights = torch.round(weight_tensor / scale).clamp(-127, 127).numpy().astype(np.int8)  # type: ignore
                
                print(f"   ‚Ä¢ {name}: {int_weights.shape} ({int_weights.size} params) [manual quantized]")
                
                quantized_weights[name] = {
                    'weights': int_weights,
                    'scale': float(scale),
                    'zero_point': 0,
                    'shape': list(int_weights.shape),
                    'dtype': 'int8'
                }
                
                total_params += int_weights.size
        
        print(f"‚úÖ Extracted {len(quantized_weights)} weight tensors")
        print(f"   ‚Ä¢ Total quantized parameters: {total_params:,}")
        print(f"   ‚Ä¢ Estimated FPGA memory: {total_params/1024:.1f} KB (BRAM capacity: 630KB)")
        
        return quantized_weights
    
    def simulate_int4_quantization(self, weights: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """Simulate INT4 quantization for FPGA deployment."""
        print("\nüé≤ Simulating INT4 quantization...")
        
        int4_weights = {}
        total_bits_saved = 0
        
        for layer_name, weight_data in weights.items():
            int8_weights = weight_data['weights']
            
            # Simulate INT4 by quantizing to 16 levels (-7 to 8)
            # Clip to INT4 range
            int4_simulated = np.clip(int8_weights, -7, 8)
            
            # Pack two INT4 values into one INT8 for storage efficiency
            flat_weights = int4_simulated.flatten()
            if len(flat_weights) % 2 == 1:
                flat_weights = np.append(flat_weights, 0)  # Pad if odd
            
            # Pack pairs of INT4 values
            packed_weights = []
            for i in range(0, len(flat_weights), 2):
                val1 = flat_weights[i] & 0xF
                val2 = flat_weights[i+1] & 0xF
                packed = (val2 << 4) | val1
                packed_weights.append(packed)
            
            int4_weights[layer_name] = {
                'weights_int4': np.array(packed_weights, dtype=np.uint8),
                'original_shape': weight_data['shape'],
                'scale': weight_data['scale'],
                'zero_point': weight_data['zero_point'],
                'packed_size': len(packed_weights)
            }
            
            original_bits = int8_weights.size * 8
            packed_bits = len(packed_weights) * 8
            bits_saved = original_bits - packed_bits
            total_bits_saved += bits_saved
            
            print(f"   ‚Ä¢ {layer_name}: {int8_weights.size} ‚Üí {len(packed_weights)} packed values")
        
        memory_saved_kb = total_bits_saved / 8 / 1024
        print(f"‚úÖ INT4 simulation completed:")
        print(f"   ‚Ä¢ Memory saved: {memory_saved_kb:.1f} KB")
        print(f"   ‚Ä¢ Compression: ~2x additional")
        
        return int4_weights
    
    def export_fpga_weights(self, int4_weights: Dict[str, np.ndarray]):
        """Export weights in FPGA-compatible C format."""
        print("\nüè≠ Exporting FPGA-compatible weights...")
        
        # Create weights.h file for HLS
        weights_file = self.save_dir / 'weights.h'
        
        with open(weights_file, 'w') as f:
            f.write("/*\n")
            f.write(" * Quantized CNN Weights for FPGA Accelerator\n")
            f.write(" * Generated from 91% accuracy model\n")
            f.write(" * Format: INT4 weights packed into INT8 arrays\n")
            f.write(" * Target: Zybo Z7-20 FPGA (630KB BRAM available)\n")
            f.write(" * Architecture: Desktop + FPGA accelerator\n")
            f.write(" */\n\n")
            f.write("#ifndef CNN_WEIGHTS_H\n")
            f.write("#define CNN_WEIGHTS_H\n\n")
            f.write("#include <ap_int.h>\n")
            f.write("#include <ap_fixed.h>\n\n")
            
            # Export each layer
            layer_count = 0
            total_memory = 0
            
            for layer_name, weight_data in int4_weights.items():
                clean_name = layer_name.replace('.', '_').replace('/', '_')
                weights_array = weight_data['weights_int4']
                shape = weight_data['original_shape']
                scale = weight_data['scale']
                
                # Write array declaration
                f.write(f"// Layer: {layer_name}\n")
                f.write(f"// Original shape: {shape}\n")
                f.write(f"// Scale factor: {scale:.6f}\n")
                f.write(f"// Packed INT4 values: {len(weights_array)}\n")
                f.write(f"const ap_uint<8> {clean_name}_weights[{len(weights_array)}] = {{\n")
                
                # Write weight values (16 per line)
                for i, weight in enumerate(weights_array):
                    if i % 16 == 0:
                        f.write("    ")
                    f.write(f"0x{weight:02X}")
                    if i < len(weights_array) - 1:
                        f.write(", ")
                    if (i + 1) % 16 == 0 or i == len(weights_array) - 1:
                        f.write("\n")
                
                f.write("};\n\n")
                
                # Write scale and metadata
                f.write(f"const float {clean_name}_scale = {scale:.6f}f;\n")
                f.write(f"const int {clean_name}_zero_point = {weight_data['zero_point']};\n")
                f.write(f"const int {clean_name}_size = {len(weights_array)};\n\n")
                
                layer_count += 1
                total_memory += len(weights_array)
            
            # Write summary information
            f.write(f"// Summary\n")
            f.write(f"#define NUM_LAYERS {layer_count}\n")
            f.write(f"#define TOTAL_WEIGHT_MEMORY {total_memory}\n")
            f.write(f"#define TOTAL_WEIGHT_KB {total_memory/1024:.1f}\n")
            f.write(f"#define FPGA_BRAM_CAPACITY_KB 630\n")
            f.write(f"#define MEMORY_UTILIZATION_PERCENT {(total_memory/1024)/630*100:.1f}\n")
            f.write(f"#define NUM_CLASSES {self.config['num_classes']}\n")
            f.write(f"#define INPUT_SIZE {self.config['input_size']}\n\n")
            f.write("#endif // CNN_WEIGHTS_H\n")
        
        print(f"‚úÖ FPGA weights exported:")
        print(f"   ‚Ä¢ File: {weights_file}")
        print(f"   ‚Ä¢ Layers: {layer_count}")
        print(f"   ‚Ä¢ Total memory: {total_memory/1024:.1f} KB")
        print(f"   ‚Ä¢ Format: Packed INT4 in INT8 arrays")
        
        # Also save metadata JSON
        metadata = {
            'model_info': {
                'original_accuracy': self.fp32_accuracy,
                'quantized_accuracy': self.quantized_accuracy,
                'accuracy_drop': self.accuracy_drop,
                'compression_ratio': self.compression_ratio,
                'quantized_size_kb': self.quantized_size_kb
            },
            'layers': {
                layer_name: {
                    'shape': weight_data['original_shape'],
                    'scale': weight_data['scale'],
                    'zero_point': weight_data['zero_point'],
                    'packed_size': weight_data['packed_size']
                }
                for layer_name, weight_data in int4_weights.items()
            },
            'fpga_config': {
                'target_device': 'Zybo Z7-20',
                'weight_format': 'INT4_PACKED',
                'activation_format': 'INT8',
                'total_memory_kb': total_memory / 1024,
                'estimated_inference_cycles': total_memory * 2  # Rough estimate
            }
        }
        
        metadata_file = self.save_dir / 'quantization_metadata.json'
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"   ‚Ä¢ Metadata: {metadata_file}")
    
    def save_quantized_model(self):
        """Save the quantized model."""
        if self.quantized_model is None:
            raise ValueError("No quantized model to save")
            
        model_file = self.save_dir / 'quantized_model.pth'
        
        torch.save({
            'model_state_dict': self.quantized_model.state_dict(),
            'model_config': {
                'num_classes': self.config['num_classes'],
                'input_size': self.config['input_size']
            },
            'quantization_info': {
                'fp32_accuracy': self.fp32_accuracy,
                'quantized_accuracy': self.quantized_accuracy,
                'accuracy_drop': self.accuracy_drop,
                'compression_ratio': self.compression_ratio,
                'quantized_size_kb': self.quantized_size_kb
            }
        }, model_file)
        
        print(f"‚úÖ Quantized model saved: {model_file}")
    
    def plot_quantization_results(self):
        """Plot quantization results and analysis."""
        _, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # Accuracy comparison
        models = ['FP32 Original', 'INT8 Quantized']
        accuracies = [self.fp32_accuracy, self.quantized_accuracy]
        colors = ['blue', 'orange']
        
        ax1.bar(models, accuracies, color=colors, alpha=0.7)
        ax1.set_ylabel('Accuracy (%)')
        ax1.set_title('Model Accuracy Comparison')
        ax1.set_ylim(80, 100)
        
        for i, acc in enumerate(accuracies):
            ax1.text(i, acc + 0.5, f'{acc:.2f}%', ha='center', fontweight='bold')
        
        # Size comparison
        original_size_mb = sum(p.numel() * 4 for p in self.model.parameters()) / (1024 * 1024)
        quantized_size_mb = self.quantized_size_kb / 1024
        
        sizes = [original_size_mb, quantized_size_mb]
        ax2.bar(models, sizes, color=colors, alpha=0.7)
        ax2.set_ylabel('Model Size (MB)')
        ax2.set_title('Model Size Comparison')
        
        for i, size in enumerate(sizes):
            ax2.text(i, size + max(sizes)*0.02, f'{size:.1f} MB', ha='center', fontweight='bold')
        
        # Compression metrics
        metrics = ['Accuracy Drop', 'Compression Ratio', 'Size Reduction']
        values = [self.accuracy_drop, self.compression_ratio, original_size_mb/quantized_size_mb]
        
        ax3.bar(range(len(metrics)), values, color=['red', 'green', 'purple'], alpha=0.7)
        ax3.set_xticks(range(len(metrics)))
        ax3.set_xticklabels(metrics, rotation=45)
        ax3.set_ylabel('Value')
        ax3.set_title('Quantization Metrics')
        
        for i, val in enumerate(values):
            ax3.text(i, val + max(values)*0.02, f'{val:.1f}{"%" if i==0 else "x"}', 
                    ha='center', fontweight='bold')
        
        # FPGA suitability assessment
        criteria = ['Accuracy', 'Size', 'Latency', 'Power']
        scores = [
            min(100, self.quantized_accuracy),  # Accuracy score
            min(100, (8000 / self.quantized_size_kb) * 10),  # Size score (target 8KB)
            85,  # Estimated latency score
            90   # Estimated power score
        ]
        
        ax4.bar(criteria, scores, color='lightblue', alpha=0.7)
        ax4.set_ylabel('FPGA Suitability Score')
        ax4.set_title('FPGA Deployment Readiness')
        ax4.set_ylim(0, 100)
        
        for i, score in enumerate(scores):
            ax4.text(i, score + 2, f'{score:.0f}', ha='center', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(self.save_dir / 'quantization_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úÖ Analysis plots saved: {self.save_dir / 'quantization_analysis.png'}")
    
    def quantize_model(self) -> bool:
        """Main quantization pipeline."""
        try:
            print("\n" + "="*80)
            print("STARTING MODEL QUANTIZATION FOR FPGA ACCELERATOR")
            print("="*80)
            print(f"Target: <1% accuracy drop, <1MB memory, INT4/INT8 precision")
            print(f"Architecture: Desktop preprocessing + FPGA CNN acceleration")
            print(f"FPGA Resources: 630KB BRAM, 1GB DDR3")
            print(f"Device: {self.device}")
            
            # Step 1: Create calibration data
            calibration_loader = self.create_calibration_loader()
            test_loader = self.create_test_loader()
            
            # Step 2: Evaluate original model
            self.fp32_accuracy = self.evaluate_model_accuracy(self.model, test_loader, "FP32 Original")
            
            # Step 3: Apply quantization
            self.quantized_model = self.apply_post_training_quantization(calibration_loader)
            
            # Quantized model created
            
            # Step 4: Evaluate quantized model
            self.quantized_accuracy = self.evaluate_model_accuracy(self.quantized_model, test_loader, "INT8 Quantized")
            
            # Step 5: Calculate metrics
            self.accuracy_drop = self.fp32_accuracy - self.quantized_accuracy
            
            print(f"\nüìä QUANTIZATION RESULTS:")
            print(f"   ‚Ä¢ Original accuracy: {self.fp32_accuracy:.2f}%")
            print(f"   ‚Ä¢ Quantized accuracy: {self.quantized_accuracy:.2f}%")
            print(f"   ‚Ä¢ Accuracy drop: {self.accuracy_drop:.2f}%")
            print(f"   ‚Ä¢ Compression ratio: {self.compression_ratio:.1f}x")
            print(f"   ‚Ä¢ Quantized size: {self.quantized_size_kb:.1f} KB")
            
            # Check if quantization meets targets
            accuracy_ok = self.accuracy_drop <= self.config['max_accuracy_drop']
            size_ok = self.quantized_size_kb <= self.config['max_size_kb']
            
            if accuracy_ok and size_ok:
                print(f"‚úÖ QUANTIZATION SUCCESSFUL!")
            else:
                print(f"‚ö†Ô∏è  QUANTIZATION CONCERNS:")
                if not accuracy_ok:
                    print(f"   ‚Ä¢ Accuracy drop too high: {self.accuracy_drop:.2f}% > {self.config['max_accuracy_drop']}%")
                if not size_ok:
                    print(f"   ‚Ä¢ Model too large: {self.quantized_size_kb:.1f} KB > {self.config['max_size_kb']} KB")
            
            # Step 6: Extract and simulate INT4 weights
            quantized_weights = self.extract_quantized_weights()
            int4_weights = self.simulate_int4_quantization(quantized_weights)
            
            # Step 7: Export for FPGA
            self.export_fpga_weights(int4_weights)
            
            # Step 8: Save model and analysis
            self.save_quantized_model()
            self.plot_quantization_results()
            
            print(f"\n" + "="*80)
            print("QUANTIZATION COMPLETED!")
            print("="*80)
            print(f"‚úÖ Ready for FPGA accelerator deployment")
            print(f"‚úÖ Next step: HLS implementation with generated weights.h")
            print(f"‚úÖ Desktop handles: Camera, preprocessing, UI")
            print(f"‚úÖ FPGA accelerates: CNN inference only")
            
            return True
            
        except Exception as e:
            logger.error(f"Quantization failed: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """Main quantization function."""
    parser = argparse.ArgumentParser(description='Quantize trained model for FPGA deployment')
    parser.add_argument('--model_checkpoint', '-m', type=str,
                       default='model/checkpoints_fixed/best_model.pth',
                       help='Path to trained model checkpoint')
    parser.add_argument('--annotations', '-a', type=str,
                       default='data/annotations/hagrid_all_gestures_annotations.json',
                       help='Path to annotations file')
    parser.add_argument('--dataset_root', '-d', type=str,
                       default='data/local_dataset/hagrid_500k',
                       help='Root directory of dataset')
    parser.add_argument('--save_dir', '-s', type=str,
                       default='model/quantized',
                       help='Directory to save quantized model')
    parser.add_argument('--calibration_images', '-c', type=int, default=100,
                       help='Images per class for calibration')
    parser.add_argument('--max_accuracy_drop', '-t', type=float, default=1.0,
                       help='Maximum acceptable accuracy drop (%)')
    parser.add_argument('--max_size_mb', type=float, default=1.0,
                       help='Maximum model size in MB for FPGA BRAM')
    parser.add_argument('--aggressive', action='store_true',
                       help='Use aggressive quantization settings')
    
    args = parser.parse_args()
    
    config = {
        'model_checkpoint': args.model_checkpoint,
        'annotations_file': args.annotations,
        'dataset_root': args.dataset_root,
        'save_dir': args.save_dir,
        'num_classes': 18,
        'input_size': 224,
        'calibration_images_per_class': args.calibration_images,
        'calibration_batch_size': 32,
        'test_batch_size': 64,
        'max_calibration_batches': 50,
        'max_accuracy_drop': args.max_accuracy_drop,
        'max_size_kb': args.max_size_mb * 1024,  # Convert MB to KB
        'aggressive_quantization': args.aggressive
    }
    
    print("Model Quantization for FPGA Accelerator")
    print("="*50)
    print(f"Model checkpoint: {config['model_checkpoint']}")
    print(f"Annotations: {config['annotations_file']}")
    print(f"Dataset root: {config['dataset_root']}")
    print(f"Save directory: {config['save_dir']}")
    print(f"Calibration images per class: {config['calibration_images_per_class']}")
    print(f"Max accuracy drop: {config['max_accuracy_drop']}%")
    print(f"Target size: {config['max_size_kb']/1024:.1f} MB (FPGA BRAM: 630KB)")
    print(f"Aggressive quantization: {config['aggressive_quantization']}")
    print(f"Architecture: Desktop + FPGA accelerator")
    
    # Confirm quantization
    response = input("\nStart quantization? (y/N): ").strip().lower()
    if response not in ['y', 'yes']:
        print("Quantization cancelled.")
        return
    
    # Create quantizer and start
    quantizer = ModelQuantizer(config)
    success = quantizer.quantize_model()
    
    if success:
        print(f"\nüéâ SUCCESS! Model quantized and ready for FPGA deployment")
        print(f"üìÅ Files generated in: {config['save_dir']}")
        print(f"üîß Next step: HLS implementation using weights.h")
    else:
        print(f"\n‚ùå Quantization failed! Check the logs above.")


if __name__ == "__main__":
    main()